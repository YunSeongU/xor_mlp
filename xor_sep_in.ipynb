{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([[0,0],[0,1],[1,0],[1,1]])#xor input\n",
    "y0=np.array([[0],[1],[1],[0]])#xor output\n",
    "\n",
    "\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "global pm_output, pm_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_hidden1():\n",
    "    global pm_output, pm_hidden\n",
    "    pm_hidden = alloc_param_pair([2, 10])#input,hidden  weight shape:2x10, bias:1x10\n",
    "    pm_output = alloc_param_pair([10, 1])#hidden,output  weight shape:10x1, bias:1x1\n",
    "    \n",
    "def alloc_param_pair(shape):\n",
    "    weight = np.random.rand(shape[0],shape[1])# shape[0] x shape[1] size의 0~1사이 난수 생성\n",
    "    bias = np.zeros(shape[-1])\n",
    "    return {'w':weight, 'b':bias} #딕셔너리 형태로 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_neuralnet_hidden1(x): #순전파\n",
    "    global pm_output, pm_hidden\n",
    "    x=x.reshape(1,2)\n",
    "    hidden = relu(np.matmul(x, pm_hidden['w']) + pm_hidden['b']) # 1x2 * 2x10 = 1 x 10 \n",
    "    output = np.matmul(hidden, pm_output['w']) + pm_output['b'] #1x10 * 10x1 = 1x1\n",
    "    \n",
    "    return output, [x, hidden]\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0) #배열 요소의 요소별 최대값 0 if x<0 else x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_neuralnet_hidden1(G_output, aux):\n",
    "    global pm_output, pm_hidden\n",
    "    \n",
    "    x, hidden = aux #출력층,은닉측 가중치 편미분 정보\n",
    "    #ㅡㅡㅡㅡㅡ출력층 역전파 // 손실기울기 계산ㅡㅡㅡㅡㅡ\n",
    "    g_output_w_out = hidden.transpose()#출력부의 입력(히든층의 출력).transpose()\n",
    "    G_w_out = np.matmul(g_output_w_out, G_output)#(입력)T x (Loss에서 들어온 미분값) ->> 가중치 손실 미분값            \n",
    "    G_b_out = np.sum(G_output, axis=0)# 편향벡터 손실 미분값 ==> 열방향 합                   \n",
    "    #ㅡㅡㅡㅡㅡ히든층 역전파ㅡㅡㅡㅡㅡ\n",
    "    g_output_hidden = pm_output['w'].transpose()#출력층의 가중치.transpose // 가중치 업데이트 하기전에 가져오기             \n",
    "    G_hidden = np.matmul(G_output, g_output_hidden)#입력(히든층의 출력)에 대한 손실 미분값     \n",
    "    #ㅡㅡㅡㅡㅡ출력층 업데이트ㅡㅡㅡㅡㅡ\n",
    "    pm_output['w'] -= LEARNING_RATE * G_w_out  # 가중치 업데이트           \n",
    "    pm_output['b'] -= LEARNING_RATE * G_b_out  # 편향 업데이트\n",
    "    #ㅡㅡㅡㅡㅡ히든층 역전파ㅡㅡㅡㅡㅡ\n",
    "    G_hidden = G_hidden * relu_derv(hidden) #ReLU 도함수 1or0 반환 -->g_hidden은 자기자신을 반환하던가 0이 반환됨\n",
    "    g_hidden_w_hid = x.transpose()# 최초 입력 .transpose()                        \n",
    "    G_w_hid = np.matmul(g_hidden_w_hid, G_hidden) #(입력)T x (출력층에서 들어온 미분값) ->> 가중치 손실 미분값           \n",
    "    G_b_hid = np.sum(G_hidden, axis=0) #편향벡터 손실 미분값 ==> 열방향 합                       \n",
    "    #ㅡㅡㅡㅡㅡ히든층 업데이트ㅡㅡㅡㅡㅡ\n",
    "    pm_hidden['w'] -= LEARNING_RATE * G_w_hid   # 가중치 업데이트              \n",
    "    pm_hidden['b'] -= LEARNING_RATE * G_b_hid   # 편향 업데이트             \n",
    "    \n",
    "def relu_derv(y): #ReLU의 도함수는 1 if y>0 else 0 이어야 하지만\n",
    "    return np.sign(y) #y의 자리에는 활성화 함수 ReLU를 지난 hidden이 들어감으로 np.sign이 ReLU의 도함수 역할을 한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_postproc(output, y): #MSE \n",
    "    diff = output - y #출력과 정답과의 편차\n",
    "    square = np.square(diff) #편차의 제곱\n",
    "    loss = np.mean(square) #제곱된 편차의 평균\n",
    "    return loss, diff\n",
    "\n",
    "def backprop_postproc(G_loss, diff):\n",
    "    return 2*diff/np.prod(diff.shape) #MSE 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.25762983971117\n",
      "0.11147008661219282\n",
      "0.016830535186428554\n",
      "0.002583211262319417\n",
      "0.0004936789344231788\n",
      "9.320996123891057e-05\n",
      "1.7771073896680038e-05\n",
      "3.3581645696713215e-06\n",
      "6.418997792995645e-07\n",
      "1.2085274035292968e-07\n",
      "2.309751647580418e-08\n"
     ]
    }
   ],
   "source": [
    "init_model_hidden1() #히든층의 크기와 임의의 w,b 설정\n",
    "for e in range(10001): #순전파와 역전파의 반복\n",
    "    for i in range(len(x0)):\n",
    "        output, aux_nn = forward_neuralnet_hidden1(x0[i]) #순전파 진행 :: output은 최종xw+b , aux_nn는 최초입력 x와 히든층에서 ReLU(wx+b)\n",
    "        loss, aux_pp = forward_postproc(output, y0[i]) # MSE계산 :: loss는 mse값, diff는 편차\n",
    "        G_loss = 1.0 # 가장 마지막에서 들어가는 값 // 역전파 시작하는 값\n",
    "        G_output = backprop_postproc(G_loss, aux_pp) # loss의 미분\n",
    "        backprop_neuralnet_hidden1(G_output, aux_nn) # loss의 미분값을 네트워크에 전달\n",
    "    if e % 1000==0:\n",
    "        print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00015198]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output #최종학습된 가중치 및 편향에 x0를 대입한값 // hypothesis = np.matmul(relu(np.matmul(x0, pm_hidden['w']) + pm_hidden['b']), pm_output['w'])+pm_output['b']\n",
    "#output[0],output[3]은 0에 가까운값\n",
    "#output[1],output[2]은 1에 가까운값 으로\n",
    "#threshold를 0.5로 적용한다면 y와 같은값 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 xor 0 ==>: predict : [0.] \n",
      "0 xor 1 ==>: predict : [1.] \n",
      "1 xor 0 ==>: predict : [1.] \n",
      "1 xor 1 ==>: predict : [0.] \n"
     ]
    }
   ],
   "source": [
    "def pred(x):\n",
    "    return np.matmul(relu(np.matmul(x,pm_hidden['w'])+pm_hidden['b']),pm_output['w'])+pm_output['b']\n",
    "for xx in x0:\n",
    "    print('{} xor {} ==> y = {} : predict : {} '.format(xx[0],xx[1],(pred(xx)>0.5).astype(np.float64)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20221226env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb216129a813ba247032856bc56e8d94c5a1daee7b8b4cb2b6f779739e7bbeda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
