{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([[0,0],[0,1],[1,0],[1,1]])#xor input\n",
    "y0=np.array([[0],[1],[1],[0]])#xor output\n",
    "\n",
    "\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "global pm_output, pm_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_hidden1():\n",
    "    global pm_output, pm_hidden\n",
    "    \n",
    "    pm_hidden = alloc_param_pair([2, 10])#input,hidden  weight shape:2x10, bias:1x10\n",
    "    pm_output = alloc_param_pair([10, 1])#hidden,output  weight shape:10x1, bias:1x1\n",
    "    \n",
    "def alloc_param_pair(shape):\n",
    "    weight = np.random.rand(shape[0],shape[1])# shape[0] x shape[1] size의 0~1사이 난수 생성\n",
    "    bias = np.zeros(shape[-1])\n",
    "    return {'w':weight, 'b':bias} #딕셔너리 형태로 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_neuralnet_hidden1(x): #순전파\n",
    "    global pm_output, pm_hidden\n",
    "    hidden = relu(np.matmul(x, pm_hidden['w']) + pm_hidden['b']) # 4x2 * 2x10 = 4 x 10 \n",
    "    output = np.matmul(hidden, pm_output['w']) + pm_output['b'] #4x10 * 10x1 = 4x1\n",
    "    \n",
    "    return output, [x, hidden]\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0) #배열 요소의 요소별 최대값 0 if x<0 else x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_neuralnet_hidden1(G_output, aux):\n",
    "    global pm_output, pm_hidden\n",
    "    \n",
    "    x, hidden = aux #출력층,은닉측 가중치 편미분 정보\n",
    "    #ㅡㅡㅡㅡㅡ출력층 역전파 // 손실기울기 계산ㅡㅡㅡㅡㅡ\n",
    "    g_output_w_out = hidden.transpose()#출력부의 입력(히든층의 출력).transpose()\n",
    "    G_w_out = np.matmul(g_output_w_out, G_output)#(입력)T x (Loss에서 들어온 미분값) ->> 가중치 손실 미분값            \n",
    "    G_b_out = np.sum(G_output, axis=0)# 편향벡터 손실 미분값 ==> 열방향 합                   \n",
    "    #ㅡㅡㅡㅡㅡ히든층 역전파ㅡㅡㅡㅡㅡ\n",
    "    g_output_hidden = pm_output['w'].transpose()#출력층의 가중치.transpose // 가중치 업데이트 하기전에 가져오기             \n",
    "    G_hidden = np.matmul(G_output, g_output_hidden)#입력(히든층의 출력)에 대한 손실 미분값     \n",
    "    #ㅡㅡㅡㅡㅡ출력층 업데이트ㅡㅡㅡㅡㅡ\n",
    "    pm_output['w'] -= LEARNING_RATE * G_w_out  # 가중치 업데이트           \n",
    "    pm_output['b'] -= LEARNING_RATE * G_b_out  # 편향 업데이트\n",
    "    #ㅡㅡㅡㅡㅡ히든층 역전파ㅡㅡㅡㅡㅡ\n",
    "    G_hidden = G_hidden * relu_derv(hidden) #ReLU 도함수 1or0 반환 -->g_hidden은 자기자신을 반환하던가 0이 반환됨\n",
    "    g_hidden_w_hid = x.transpose()# 최초 입력 .transpose()                        \n",
    "    G_w_hid = np.matmul(g_hidden_w_hid, G_hidden) #(입력)T x (출력층에서 들어온 미분값) ->> 가중치 손실 미분값           \n",
    "    G_b_hid = np.sum(G_hidden, axis=0) #편향벡터 손실 미분값 ==> 열방향 합                       \n",
    "    #ㅡㅡㅡㅡㅡ히든층 업데이트ㅡㅡㅡㅡㅡ\n",
    "    pm_hidden['w'] -= LEARNING_RATE * G_w_hid   # 가중치 업데이트              \n",
    "    pm_hidden['b'] -= LEARNING_RATE * G_b_hid   # 편향 업데이트             \n",
    "    \n",
    "def relu_derv(y): #ReLU의 도함수는 1 if y>0 else 0 이어야 하지만\n",
    "    return np.sign(y) #y의 자리에는 활성화 함수 ReLU를 지난 hidden이 들어감으로 np.sign이 ReLU의 도함수 역할을 한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_postproc(output, y): #MSE \n",
    "    diff = output - y #출력과 정답과의 편차\n",
    "    square = np.square(diff) #편차의 제곱\n",
    "    loss = np.mean(square) #제곱된 편차의 평균\n",
    "    return loss, diff\n",
    "\n",
    "def backprop_postproc(G_loss, diff):\n",
    "    return 2*diff/np.prod(diff.shape) #MSE 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3694040233579803\n",
      "0.22211512678264772\n",
      "0.12711634527701735\n",
      "0.08081606277369754\n",
      "0.049763892003518224\n",
      "0.029191056204670218\n",
      "0.016845839576249532\n",
      "0.008964695553467123\n",
      "0.004323243309547941\n",
      "0.0022696677912744856\n",
      "0.0012154394944983763\n"
     ]
    }
   ],
   "source": [
    "init_model_hidden1() #히든층의 크기와 임의의 w,b 설정\n",
    "for e in range(10001): #순전파와 역전파의 반복\n",
    "    output, aux_nn = forward_neuralnet_hidden1(x0) #순전파 진행 :: output은 최종xw+b , aux_nn는 최초입력 x와 히든층에서 ReLU(wx+b)\n",
    "    loss, aux_pp = forward_postproc(output, y0) # MSE계산 :: loss는 mse값, diff는 편차\n",
    "    G_loss = 1.0 # 가장 마지막에서 들어가는 값 // 역전파 시작하는 값\n",
    "    G_output = backprop_postproc(G_loss, aux_pp) # loss의 미분\n",
    "    backprop_neuralnet_hidden1(G_output, aux_nn) # loss의 미분값을 네트워크에 전달\n",
    "    if e % 1000==0:\n",
    "        print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0539624 ],\n",
       "       [0.96986064],\n",
       "       [0.97646705],\n",
       "       [0.0220825 ]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output #최종학습된 가중치 및 편향에 x0를 대입한값 // hypothesis = np.matmul(relu(np.matmul(x0, pm_hidden['w']) + pm_hidden['b']), pm_output['w'])+pm_output['b']\n",
    "#output[0],output[3]은 0에 가까운값\n",
    "#output[1],output[2]은 1에 가까운값 으로\n",
    "#threshold를 0.5로 적용한다면 y와 같은값 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = (output>0.5).astype(np.float64)\n",
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20221226env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb216129a813ba247032856bc56e8d94c5a1daee7b8b4cb2b6f779739e7bbeda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
